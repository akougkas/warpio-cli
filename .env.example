# Warpio CLI Configuration Examples
# Copy this file to .env and configure your preferred AI providers

# =============================================================================
# GEMINI (Default Provider) - Google AI Studio
# =============================================================================
# Get your API key from: https://aistudio.google.com/app/apikey

GEMINI_API_KEY=your_gemini_api_key_here
# Optional: Specify Gemini model (defaults to gemini-1.5-flash-latest)
# GEMINI_MODEL=gemini-2.0-flash-exp

# =============================================================================
# LM STUDIO (Local AI Models)
# =============================================================================
# Run LM Studio locally and start the local server
# Configure your local endpoint and model

# WARPIO_PROVIDER=lmstudio
# LMSTUDIO_HOST=http://localhost:1234/v1
# LMSTUDIO_MODEL=qwen3-4b-instruct-2507
# LMSTUDIO_API_KEY=lm-studio

# Popular LM Studio Models:
# - qwen3-4b-instruct-2507 (4B parameters, good for development)
# - gpt-oss-20b (20B parameters, more capable)
# - llama-3.2-3b-instruct (3B parameters, lightweight)
# - phi-3.5-mini-instruct (3.8B parameters, Microsoft)

# =============================================================================
# OLLAMA (Local AI Models)
# =============================================================================
# Install and run Ollama locally: https://ollama.ai
# Pull models with: ollama pull <model-name>

# WARPIO_PROVIDER=ollama
# OLLAMA_HOST=http://localhost:11434
# OLLAMA_MODEL=qwen2.5-coder:7b
# OLLAMA_API_KEY=ollama

# Popular Ollama Models:
# - qwen2.5-coder:7b (7B parameters, coding focused)
# - llama3.2:3b (3B parameters, general purpose)
# - phi3.5:3.8b (3.8B parameters, Microsoft)
# - deepseek-coder:6.7b (6.7B parameters, coding specialized)
# - mistral:7b (7B parameters, general purpose)

# =============================================================================
# OPENAI (Cloud API)
# =============================================================================
# Get your API key from: https://platform.openai.com/api-keys

# WARPIO_PROVIDER=openai
# OPENAI_API_KEY=sk-your_openai_api_key_here
# OPENAI_MODEL=gpt-4o-mini
# OPENAI_BASE_URL=https://api.openai.com/v1

# Available OpenAI Models:
# - gpt-4o (most capable, multimodal)
# - gpt-4o-mini (fast and cost-effective)
# - gpt-4-turbo (fast GPT-4 variant)
# - gpt-3.5-turbo (legacy, cost-effective)

# =============================================================================
# MODEL SELECTION EXAMPLES
# =============================================================================
# You can override the provider/model using CLI arguments:

# Use Gemini 2.0 Flash:
#   npx warpio -m gemini::gemini-2.0-flash-exp -p "Hello"

# Use LM Studio with Qwen model:
#   npx warpio -m lmstudio::qwen3-4b-instruct-2507 -p "Hello"

# Use Ollama with CodeLlama:
#   npx warpio -m ollama::qwen2.5-coder:7b -p "Hello"

# Use OpenAI GPT-4:
#   npx warpio -m openai::gpt-4o-mini -p "Hello"

# =============================================================================
# PERSONA EXAMPLES
# =============================================================================
# Combine providers with personas for specialized workflows:

# Data science with local model:
#   WARPIO_PROVIDER=lmstudio npx warpio --persona data-expert -p "Convert CSV to Parquet"

# HPC analysis with Ollama:
#   WARPIO_PROVIDER=ollama npx warpio --persona hpc-expert -p "Optimize MPI code"

# Research with Gemini:
#   npx warpio --persona research-expert -p "Summarize latest ML papers"

# =============================================================================
# DEBUGGING & TROUBLESHOOTING
# =============================================================================
# Uncomment these for debugging provider issues:

# Debug mode (shows detailed logs):
# DEBUG=warpio:*

# Test provider connections:
# Run: npx warpio /model test

# List available models:
# Run: npx warpio /model list

# Show current configuration:
# Run: npx warpio /model current

# =============================================================================
# ADVANCED CONFIGURATION
# =============================================================================
# These are optional and use sensible defaults if not specified:

# Sandbox mode (if supported by provider):
# GEMINI_SANDBOX=true

# Custom timeouts and retries:
# WARPIO_TIMEOUT=30000
# WARPIO_MAX_RETRIES=3

# Custom model parameters (provider-specific):
# LMSTUDIO_TEMPERATURE=0.7
# LMSTUDIO_MAX_TOKENS=2048
# OLLAMA_TEMPERATURE=0.8
# OPENAI_TEMPERATURE=0.9

# =============================================================================
# QUICK START GUIDE
# =============================================================================
# 1. Choose your provider above and uncomment the relevant section
# 2. Copy this file to .env: cp .env.example .env
# 3. Add your API keys and configure settings
# 4. Test your setup: npx warpio -p "Hello, can you help me?"
# 5. Use slash commands: /model list, /model current, /model test

# Need help? Check the documentation:
# - GitHub: https://github.com/your-username/warpio-cli
# - Personas: npx warpio --list-personas
# - Models: npx warpio /model list